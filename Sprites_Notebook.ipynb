{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9a+4BAVwIhgivBshbG8pb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Osmanoor/Sprites/blob/main/Sprites_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Section 1: Setup and Hyperparameters**\n",
        "\n",
        "First, we set up the environment. We define the specific hyperparameters used in the course to generate the 16x16 sprites."
      ],
      "metadata": {
        "id": "x0MIiK8prrxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mb3SHnUrgk3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import os\n",
        "from diffusion_utilities import *  # Imports the helper functions you provided\n",
        "\n",
        "# Define the device (Use GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "\n",
        "# Diffusion hyperparameters\n",
        "timesteps = 500              # How many steps to add/remove noise\n",
        "beta1 = 1e-4                 # Start of the noise schedule\n",
        "beta2 = 0.02                 # End of the noise schedule\n",
        "\n",
        "# Network hyperparameters\n",
        "n_feat = 64                  # Hidden dimension feature size\n",
        "n_cfeat = 5                  # Context vector size (Hero, Non-Hero, Food, Spell, Side-facing)\n",
        "height = 16                  # Image size (16x16)\n",
        "save_dir = './weights/'      # Where to save models\n",
        "\n",
        "# Training hyperparameters\n",
        "batch_size = 100\n",
        "n_epoch = 32\n",
        "lrate = 1e-3\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Section 2: The Forward Diffusion Process**\n",
        "\n",
        "Here we define the **Noise Schedule**. This is the mathematical formula that decides how much noise to add at every timestep $t$.\n",
        "\n",
        "*   $\\beta_t$ (`b_t`): The variance of noise added at step $t$.\n",
        "*   $\\alpha_t$ (`a_t`): $1 - \\beta_t$.\n",
        "*   $\\bar{\\alpha}_t$ (`ab_t`): The cumulative product of $\\alpha$. This allows us to jump directly from a clean image ($x_0$) to a noisy image at any step ($x_t$) without iterating."
      ],
      "metadata": {
        "id": "44NONRP_r9t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct DDPM noise schedule\n",
        "# We use a linear schedule for beta, increasing from beta1 to beta2\n",
        "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
        "a_t = 1 - b_t\n",
        "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
        "ab_t[0] = 1\n",
        "\n",
        "def perturb_input(x, t, noise):\n",
        "    \"\"\"\n",
        "    Perturbs an image 'x' to a specific timestep 't' using the noise schedule.\n",
        "    Formula: sqrt(alpha_bar_t) * x + sqrt(1 - alpha_bar_t) * noise\n",
        "    \"\"\"\n",
        "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
      ],
      "metadata": {
        "id": "FehaqzQEsS-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Section 3: The Neural Network (ContextUnet)**\n",
        "\n",
        "This is the core brain of the operation. We use the helper classes (`ResidualConvBlock`, `UnetUp`, `UnetDown`, `EmbedFC`) from your `diffusion_utilities.py` file to assemble the full U-Net.\n",
        "\n",
        "**Key Features of this Architecture:**\n",
        "1.  **Same Output Size:** Input is 16x16, Output is 16x16 (predicting noise).\n",
        "2.  **Time Embeddings:** We inject the timestep $t$ so the model knows \"how noisy\" the image currently is.\n",
        "3.  **Context Embeddings:** We inject information about *what* to generate (e.g., \"Hero\" vs \"Spell\")."
      ],
      "metadata": {
        "id": "dY7VOWSPsY_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextUnet(nn.Module):\n",
        "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):\n",
        "        super(ContextUnet, self).__init__()\n",
        "\n",
        "        # input channels, intermediate features, context features\n",
        "        self.in_channels = in_channels\n",
        "        self.n_feat = n_feat\n",
        "        self.n_cfeat = n_cfeat\n",
        "        self.h = height\n",
        "\n",
        "        # Initialize the initial convolutional layer\n",
        "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
        "\n",
        "        # Initialize the down-sampling path of the U-Net with two levels\n",
        "        self.down1 = UnetDown(n_feat, n_feat)        # dimensions: [Batch, 64, 8, 8]\n",
        "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # dimensions: [Batch, 128, 4, 4]\n",
        "\n",
        "        # Bottleneck: map spatial features to a vector\n",
        "        # self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU()) # Original for 28x28\n",
        "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU()) # Adjusted for 16x16 input\n",
        "\n",
        "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
        "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
        "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
        "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
        "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
        "\n",
        "        # Initialize the up-sampling path of the U-Net with three levels\n",
        "        self.up0 = nn.Sequential(\n",
        "            # up-sample\n",
        "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4),\n",
        "            nn.GroupNorm(8, 2 * n_feat), # normalize\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
        "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
        "\n",
        "        # Initialize the final convolutional layers to map to the same number of channels as the input\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps\n",
        "            nn.GroupNorm(8, n_feat),                # normalize\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, c=None):\n",
        "        \"\"\"\n",
        "        x : (batch, n_feat, h, w) : input image\n",
        "        t : (batch, n_cfeat)      : time step\n",
        "        c : (batch, n_classes)    : context label\n",
        "        \"\"\"\n",
        "        # x is the input image, c is the context label, t is the timestep\n",
        "\n",
        "        # Pass the input image through the initial convolutional layer\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        # Pass the result through the down-sampling path\n",
        "        down1 = self.down1(x)       # [Batch, 64, 8, 8]\n",
        "        down2 = self.down2(down1)   # [Batch, 128, 4, 4]\n",
        "\n",
        "        # Convert the feature maps to a vector and apply an activation\n",
        "        hiddenvec = self.to_vec(down2)\n",
        "\n",
        "        # Mask out context if context_mask == 1 (or if c is None)\n",
        "        if c is None:\n",
        "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
        "\n",
        "        # Embed context and timestep\n",
        "        # We view/reshape them to match the spatial dimensions for broadcasting or concatenation\n",
        "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
        "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
        "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
        "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
        "\n",
        "        # Up-sampling path\n",
        "        up1 = self.up0(hiddenvec)\n",
        "\n",
        "        # For up2, we add the embeddings to the output of up1 before feeding into UnetUp\n",
        "        # This is where the control happens!\n",
        "        up2 = self.up1(cemb1 * up1 + temb1, down2)  # Add and multiply embeddings\n",
        "        up3 = self.up2(cemb2 * up2 + temb2, down1)\n",
        "\n",
        "        # Concatenate with original x (skip connection) and output\n",
        "        out = self.out(torch.cat((up3, x), 1))\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)\n",
        "print(f\"Model instantiated. Number of parameters: {sum(p.numel() for p in nn_model.parameters())}\")"
      ],
      "metadata": {
        "id": "UdCh6cquscgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here is the next part of the Unified Notebook.\n",
        "\n",
        "### **Section 4: The Training Loop**\n",
        "\n",
        "In this section, we define how the model learns.\n",
        "\n",
        "**Key Concepts from the Course:**\n",
        "1.  **Random Time Sampling:** We don't train on just one noise level. For every image in a batch, we pick a random timestep $t$ (e.g., image 1 gets $t=50$, image 2 gets $t=400$).\n",
        "2.  **Context Masking:** To make the model robust (and capable of generating images without specific instructions), we randomly hide the context label (set it to zero) about 10% of the time during training.\n",
        "3.  **Objective:** The model tries to predict the **noise** that was added, not the image itself.\n",
        "\n",
        "*Note: Training this on a CPU takes hours. I have set `train_model = False` by default. If you have the pre-trained weights (`model_trained.pth` or `context_model_trained.pth`) in the `save_dir`, the code will skip training and load them.*"
      ],
      "metadata": {
        "id": "XTBdPmbrsioa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Dataset ---\n",
        "# We assume the files are in the current directory as per the course structure\n",
        "# You can change these paths if your data is elsewhere\n",
        "dataset = CustomDataset(\n",
        "    \"./sprites_1788_16x16.npy\",\n",
        "    \"./sprite_labels_nc_1788_16x16.npy\",\n",
        "    transform,\n",
        "    null_context=False\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)\n",
        "\n",
        "# --- Training Logic ---\n",
        "\n",
        "train_model = False  # Set to True if you want to train from scratch (takes time!)\n",
        "\n",
        "if train_model:\n",
        "    nn_model.train()\n",
        "\n",
        "    for ep in range(n_epoch):\n",
        "        print(f'Epoch {ep}')\n",
        "\n",
        "        # Linearly decay learning rate\n",
        "        optim.param_groups[0]['lr'] = lrate * (1 - ep / n_epoch)\n",
        "\n",
        "        # Progress bar\n",
        "        pbar = tqdm(dataloader, mininterval=2)\n",
        "\n",
        "        for x, c in pbar:  # x: images, c: context labels\n",
        "            optim.zero_grad()\n",
        "            x = x.to(device)\n",
        "            c = c.to(x)\n",
        "\n",
        "            # --- Context Masking ---\n",
        "            # Randomly mask out context to allow unconditional generation\n",
        "            # Bernoulli(0.9) means 90% chance of keeping context, 10% chance of masking\n",
        "            context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.9).to(device)\n",
        "            c = c * context_mask.unsqueeze(-1)\n",
        "\n",
        "            # --- Perturb Data ---\n",
        "            noise = torch.randn_like(x)\n",
        "            t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device)\n",
        "            x_pert = perturb_input(x, t, noise)\n",
        "\n",
        "            # --- Forward Pass ---\n",
        "            # Predict the noise in the image\n",
        "            # We pass t / timesteps to normalize the time input to [0, 1]\n",
        "            pred_noise = nn_model(x_pert, t / timesteps, c=c)\n",
        "\n",
        "            # --- Loss Calculation ---\n",
        "            # Loss is Mean Squared Error between predicted noise and actual noise\n",
        "            loss = F.mse_loss(pred_noise, noise)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "        # Save model periodically\n",
        "        if ep % 4 == 0 or ep == int(n_epoch - 1):\n",
        "            torch.save(nn_model.state_dict(), save_dir + f\"context_model_{ep}.pth\")\n",
        "            print('Saved model at ' + save_dir + f\"context_model_{ep}.pth\")\n",
        "\n",
        "else:\n",
        "    # Load pre-trained weights if we aren't training\n",
        "    # Ensure you have the file 'context_model_trained.pth' or similar in weights/\n",
        "    try:\n",
        "        # Trying to load the final model from the course materials\n",
        "        model_path = f\"{save_dir}/context_model_trained.pth\"\n",
        "        # If that doesn't exist, try looking for a specific epoch\n",
        "        if not os.path.exists(model_path):\n",
        "            model_path = f\"{save_dir}/context_model_31.pth\"\n",
        "\n",
        "        nn_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        nn_model.eval()\n",
        "        print(f\"Loaded model weights from {model_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"No pre-trained model found. Please set train_model = True to train from scratch.\")"
      ],
      "metadata": {
        "id": "fnkG3ofOs7aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 5: Standard Sampling (DDPM)**\n",
        "\n",
        "Now we use the trained brain to hallucinate sprites. This is the **Reverse Diffusion Process**.\n",
        "\n",
        "**The Math of \"Denoise & Add Noise\":**\n",
        "We want to move from noisy $x_t$ to slightly cleaner $x_{t-1}$.\n",
        "1.  **Subtract Prediction:** We use the neural network to remove the predicted noise.\n",
        "2.  **Add Noise Back ($z$):** As explained in Video 2, if we just subtract, the images collapse into generic \"blobs.\" We must add a tiny bit of random noise back (scaled by $\\sigma_t$) to maintain texture and variety, effectively \"shaking\" the image into the correct distribution."
      ],
      "metadata": {
        "id": "EtDVGq0ttklh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function: removes the predicted noise (but adds some noise back in to avoid collapse)\n",
        "def denoise_add_noise(x, t, pred_noise, z=None):\n",
        "    if z is None:\n",
        "        z = torch.randn_like(x)\n",
        "\n",
        "    noise = b_t.sqrt()[t] * z\n",
        "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
        "    return mean + noise\n",
        "\n",
        "# Standard Sampling Algorithm (DDPM)\n",
        "@torch.no_grad()\n",
        "def sample_ddpm(n_sample, context, save_rate=20):\n",
        "    # 1. Start with pure noise x_T ~ N(0, 1)\n",
        "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
        "\n",
        "    # Array to keep track of generated steps for plotting/animation\n",
        "    intermediate = []\n",
        "\n",
        "    # 2. Iterate backwards from T to 1\n",
        "    for i in range(timesteps, 0, -1):\n",
        "        print(f'sampling timestep {i:3d}', end='\\r')\n",
        "\n",
        "        # Reshape time tensor for the model\n",
        "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
        "\n",
        "        # Sample some random noise to inject back in (z)\n",
        "        # For the very last step (i=1), we don't add noise back (z=0) because we want a clean image\n",
        "        z = torch.randn_like(samples) if i > 1 else 0\n",
        "\n",
        "        # 3. Predict noise using the UNet\n",
        "        eps = nn_model(samples, t, c=context)\n",
        "\n",
        "        # 4. Update samples using the denoise function\n",
        "        samples = denoise_add_noise(samples, i, eps, z)\n",
        "\n",
        "        # Save intermediate images for visualization\n",
        "        if i % save_rate == 0 or i == timesteps or i < 8:\n",
        "            intermediate.append(samples.detach().cpu().numpy())\n",
        "\n",
        "    intermediate = np.stack(intermediate)\n",
        "    return samples, intermediate"
      ],
      "metadata": {
        "id": "BDhfoIQZtopX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualization**\n",
        "\n",
        "Let's verify it works by generating samples with random contexts.\n"
      ],
      "metadata": {
        "id": "jZ7UTQWFtvEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize samples with randomly selected context\n",
        "plt.clf()\n",
        "\n",
        "# Create random context vectors (batch of 32)\n",
        "# randint(0, 5) picks a category, F.one_hot makes it a vector\n",
        "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
        "\n",
        "# Run sampling\n",
        "samples, intermediate = sample_ddpm(32, ctx)\n",
        "\n",
        "# Create animation\n",
        "animation_ddpm = plot_sample(intermediate, 32, 4, save_dir, \"ani_run\", None, save=False)\n",
        "\n",
        "# Display in Jupyter (if running in a notebook)\n",
        "from IPython.display import HTML\n",
        "HTML(animation_ddpm.to_jshtml())"
      ],
      "metadata": {
        "id": "zRDBopPdtzkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 6: Contextual Control (The \"Avocado Armchair\")**\n",
        "\n",
        "Now that the model is trained (or weights loaded), we can control what it generates.\n",
        "\n",
        "**How it works:**\n",
        "The model learned 5 specific categories (Hero, Non-Human, Food, Spell, Side-facing). We can pass a **One-Hot Vector** (e.g., `[1, 0, 0, 0, 0]`) to force a specific type.\n",
        "But we can also pass **Mixed Vectors** (e.g., `[0.5, 0, 0.5, 0, 0]`) to create hybrids."
      ],
      "metadata": {
        "id": "LJxmqlntt4rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to visualize a grid of images nicely\n",
        "def show_images(imgs, nrow=2):\n",
        "    _, axs = plt.subplots(nrow, imgs.shape[0] // nrow, figsize=(4,2))\n",
        "    axs = axs.flatten()\n",
        "    for img, ax in zip(imgs, axs):\n",
        "        img = (img.permute(1, 2, 0).clip(-1, 1).detach().cpu().numpy() + 1) / 2\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "# --- 1. User Defined Context (Strict Categories) ---\n",
        "print(\"Generating strict categories...\")\n",
        "\n",
        "# Define context manually:\n",
        "# 0: Hero, 1: Non-hero, 2: Food, 3: Spell, 4: Side-facing\n",
        "ctx = torch.tensor([\n",
        "    [1,0,0,0,0],  # Hero\n",
        "    [1,0,0,0,0],  # Hero\n",
        "    [0,0,0,0,1],  # Side-facing\n",
        "    [0,0,0,0,1],  # Side-facing\n",
        "    [0,1,0,0,0],  # Non-Hero\n",
        "    [0,1,0,0,0],  # Non-Hero\n",
        "    [0,0,1,0,0],  # Food\n",
        "    [0,0,1,0,0],  # Food\n",
        "]).float().to(device)\n",
        "\n",
        "# Sample using standard DDPM\n",
        "samples, _ = sample_ddpm(ctx.shape[0], ctx)\n",
        "show_images(samples)\n",
        "\n",
        "# --- 2. Mixing Context (The \"Avocado Armchair\" Effect) ---\n",
        "print(\"Generating mixed concepts...\")\n",
        "\n",
        "ctx = torch.tensor([\n",
        "    [1,0,0,0,0],      # Pure Human\n",
        "    [1,0,0.6,0,0],    # Human + Food (Edible Hero?)\n",
        "    [0,0,0.6,0.4,0],  # Food + Spell (Magic Potion?)\n",
        "    [1,0,0,0,1],      # Human + Side-facing\n",
        "    [1,1,0,0,0],      # Human + Non-Human (Hybrid Monster)\n",
        "    [1,0,0,1,0]       # Human + Spell (Wizard)\n",
        "]).float().to(device)\n",
        "\n",
        "samples, _ = sample_ddpm(ctx.shape[0], ctx)\n",
        "show_images(samples)"
      ],
      "metadata": {
        "id": "irTFcepWuBhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 7: Fast Sampling (DDIM)**\n",
        "\n",
        "Standard DDPM takes 500 steps. That's slow.\n",
        "**DDIM (Denoising Diffusion Implicit Models)** is a deterministic algorithm that allows us to skip steps. We can generate a good image in just **20 steps**.\n",
        "\n",
        "**The Math Difference:**\n",
        "DDIM doesn't add random noise $z$ at every step. Instead, it looks at the current noisy image, predicts the final clean image $x_0$, and then points the direction to the *next* noisy step $x_{t-1}$."
      ],
      "metadata": {
        "id": "_C0I8Cy0uOBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define DDIM Sampling Function ---\n",
        "\n",
        "def denoise_ddim(x, t, t_prev, pred_noise):\n",
        "    \"\"\"\n",
        "    DDIM Step: Removes noise deterministically to jump from step t to step t_prev.\n",
        "    \"\"\"\n",
        "    ab = ab_t[t]\n",
        "    ab_prev = ab_t[t_prev]\n",
        "\n",
        "    # 1. Predict the final clean image (x0) based on current prediction\n",
        "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
        "\n",
        "    # 2. Determine the direction pointing to x_t\n",
        "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
        "\n",
        "    # 3. Combine to get x_{t-1}\n",
        "    return x0_pred + dir_xt\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_ddim(n_sample, context, n=20):\n",
        "    \"\"\"\n",
        "    Fast Sampling.\n",
        "    n: Number of steps to actually take (e.g., 20 instead of 500)\n",
        "    \"\"\"\n",
        "    # Start with pure noise\n",
        "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
        "\n",
        "    intermediate = []\n",
        "\n",
        "    # Calculate step size (e.g., 500 // 20 = 25 steps per jump)\n",
        "    step_size = timesteps // n\n",
        "\n",
        "    # Iterate backwards, skipping steps\n",
        "    for i in range(timesteps, 0, -step_size):\n",
        "        print(f'DDIM sampling timestep {i:3d}', end='\\r')\n",
        "\n",
        "        # Create time tensor\n",
        "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
        "\n",
        "        # Predict noise\n",
        "        eps = nn_model(samples, t, c=context)\n",
        "\n",
        "        # Denoise using DDIM formula\n",
        "        # We jump from i -> i - step_size\n",
        "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
        "\n",
        "        intermediate.append(samples.detach().cpu().numpy())\n",
        "\n",
        "    intermediate = np.stack(intermediate)\n",
        "    return samples, intermediate"
      ],
      "metadata": {
        "id": "qkNUqZXZuT-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparison & Benchmark**\n",
        "\n",
        "Finally, let's verify the speedup on your T4 GPU. We will generate 32 images using both methods and time them."
      ],
      "metadata": {
        "id": "wmFRtw48uVBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Create a random context for the benchmark\n",
        "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
        "\n",
        "print(\"\\n--- Benchmarking Speed ---\")\n",
        "\n",
        "# 1. Time DDPM (500 Steps)\n",
        "start_time = time.time()\n",
        "sample_ddpm(32, ctx)\n",
        "ddpm_time = time.time() - start_time\n",
        "print(f\"\\nDDPM (500 steps) time: {ddpm_time:.2f} seconds\")\n",
        "\n",
        "# 2. Time DDIM (20 Steps)\n",
        "start_time = time.time()\n",
        "sample_ddim(32, ctx, n=20)\n",
        "ddim_time = time.time() - start_time\n",
        "print(f\"\\nDDIM (20 steps) time:  {ddim_time:.2f} seconds\")\n",
        "\n",
        "print(f\"\\nSpeedup Factor: {ddpm_time / ddim_time:.2f}x faster!\")\n",
        "\n",
        "# Visual Check of DDIM Quality\n",
        "print(\"\\nVisualizing DDIM output:\")\n",
        "samples, _ = sample_ddim(32, ctx, n=20)\n",
        "show_images(samples)"
      ],
      "metadata": {
        "id": "LE6dfUF8ubVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}